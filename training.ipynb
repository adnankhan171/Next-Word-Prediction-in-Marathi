{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519bc606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbfd1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 624074\n",
      "Total unique words: 67391\n",
      "Max sequence length: 30\n",
      "Total sequences generated(total samples): 576218\n",
      "Training samples: 460974\n",
      "Testing samples: 115244\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "\n",
    "try:\n",
    "    \n",
    "    with open(\"marathi_short.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'temp.txt' not found. Please create this file.\")\n",
    "    text = \"\" \n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1 \n",
    "total_tokens = sum(tokenizer.word_counts.values())\n",
    "\n",
    "input_sequences = []\n",
    "for sentence in text.split('\\n'):\n",
    "  tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
    "  for i in range(1, len(tokenized_sentence)):\n",
    "    input_sequences.append(tokenized_sentence[:i+1])\n",
    "\n",
    "\n",
    "max_len = max([len(x) for x in input_sequences]) \n",
    "padded_input_sequence = pad_sequences(input_sequences, maxlen=max_len, padding='pre')\n",
    "\n",
    "X = padded_input_sequence[:,:-1]\n",
    "y = padded_input_sequence[:,-1]\n",
    "# y = to_categorical(y, num_classes=total_words)\n",
    "total_sequences = padded_input_sequence.shape[0] \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(f\"Total Tokens: {total_tokens}\")\n",
    "print(f\"Total unique words: {total_words}\")\n",
    "print(f\"Max sequence length: {max_len}\")\n",
    "print(f\"Total sequences generated(total samples): {total_sequences}\")\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Testing samples: {X_test.shape[0]}\")\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(total_words, 100, input_length=max_len - 1),\n",
    "    LSTM(512,),\n",
    "    Dense(total_words, activation='softmax')\n",
    "])\n",
    "\n",
    "model.build(input_shape=(None, max_len - 1))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nfinal Model Evaluation\")\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"\\n Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Test Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79ae1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained Keras model (HDF5 .h5 file) and tokenizer (pickle)\n",
    "import pickle\n",
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "# -- save model --\n",
    "model.save(\"next_word_model.h5\")   # saves architecture + weights + optimizer state\n",
    "\n",
    "# -- save tokenizer --\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "print(\"Saved model -> next_word_model.h5\")\n",
    "print(\"Saved tokenizer -> tokenizer.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37502613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference: load model + tokenizer, predict next word for a Marathi seed sentence\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# load model and tokenizer\n",
    "model = load_model(\"next_word_model.h5\")\n",
    "with open(\"tokenizer.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# make sure you set this to the same window size / sequence length used for training\n",
    "max_sequence_len = 10   # <--- replace with the exact window_size/max_sequence_len used during training\n",
    "\n",
    "def predict_next_word(model, tokenizer, seed_text, max_sequence_len, top_k=1):\n",
    "    \"\"\"\n",
    "    Predict the next word for seed_text using the loaded model and tokenizer.\n",
    "    Returns:\n",
    "      - top_k list of (word, probability) tuples (length = top_k)\n",
    "    \"\"\"\n",
    "    # convert seed text to integer tokens\n",
    "    seq = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "    # pad/truncate to the model input length (pre-padding as used during training)\n",
    "    padded = pad_sequences([seq], maxlen=max_sequence_len, padding=\"pre\")\n",
    "\n",
    "    # predict probabilities for the next token\n",
    "    probs = model.predict(padded, verbose=0)[0]   # shape: (vocab_size,)\n",
    "\n",
    "    # get top_k token indices and probs\n",
    "    top_indices = probs.argsort()[-top_k:][::-1]   # descending\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        # map index -> word (tokenizer.index_word exists when tokenizer was fit)\n",
    "        word = tokenizer.index_word.get(idx, None)\n",
    "        results.append((word, float(probs[idx])))\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "seed = \"राहुल गांधी यांनी माफी\"   # replace with your Marathi seed sentence\n",
    "top_k = 3               # get top-3 predictions\n",
    "preds = predict_next_word(model, tokenizer, seed, max_sequence_len, top_k=top_k)\n",
    "\n",
    "print(\"Seed:\", seed)\n",
    "print(\"Top predictions:\")\n",
    "for word, prob in preds:\n",
    "    print(f\"  {word} \\t (prob = {prob:.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
