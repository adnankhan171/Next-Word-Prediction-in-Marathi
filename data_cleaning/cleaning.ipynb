{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6f9d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"tweets-train.csv\",encoding=\"utf-8\")\n",
    "\n",
    "summ_texts = df[\"tweet\"].dropna().astype(str).tolist()\n",
    "\n",
    "corpus = \" \".join(summ_texts)\n",
    "\n",
    "with open(\"temp1.txt\",\"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(corpus)\n",
    "    \n",
    "print(\"extraction done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61e12b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning sentences: 100%|██████████| 45637/45637 [00:00<00:00, 259087.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Saved 28189 clean Marathi sentences to 'temp2.txt'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "input_path = Path(\"temp1.txt\")   # input file\n",
    "output_path = Path(\"temp2.txt\")  # output file\n",
    "\n",
    "# ---------- COMPILE REGEX ----------\n",
    "# Match only Marathi (Devanagari) words\n",
    "marathi_pattern = re.compile(r'\\p{Devanagari}+')\n",
    "# Split text by full stop and optional spaces\n",
    "sentence_splitter = re.compile(r'\\.\\s*')\n",
    "\n",
    "# ---------- STEP 1: Read text ----------\n",
    "text = input_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "# ---------- STEP 2: Split into sentences ----------\n",
    "sentences = sentence_splitter.split(text)\n",
    "\n",
    "cleaned_sentences = []\n",
    "\n",
    "# ---------- STEP 3: Filter Marathi text efficiently ----------\n",
    "for sentence in tqdm(sentences, desc=\"Cleaning sentences\"):\n",
    "    # Extract all Marathi sequences directly\n",
    "    marathi_words = marathi_pattern.findall(sentence)\n",
    "    if marathi_words:\n",
    "        cleaned_sentences.append(\" \".join(marathi_words))\n",
    "\n",
    "# ---------- STEP 4: Write output ----------\n",
    "output_path.write_text(\"\\n\".join(cleaned_sentences), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ Done! Saved {len(cleaned_sentences)} clean Marathi sentences to '{output_path.name}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b636f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "text = \"राम घरी जातो\"\n",
    "\n",
    "for i in range(10):\n",
    "  # tokenize\n",
    "  token_text = tokenizer.texts_to_sequences([text])[0]\n",
    "  # padding\n",
    "  padded_token_text = pad_sequences([token_text], maxlen=56, padding='pre')\n",
    "  # predict\n",
    "  pos = np.argmax(model.predict(padded_token_text))\n",
    "\n",
    "  for word,index in tokenizer.word_index.items():\n",
    "    if index == pos:\n",
    "      text = text + \" \" + word\n",
    "      print(text)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7ea26ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering sentences: 100%|██████████| 50724/50724 [00:00<00:00, 507747.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Cleaned sentences saved to 'marathi_short.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "input_file = \"marathi_clean.txt\"      # source file\n",
    "output_file = \"marathi_short.txt\"     # destination file\n",
    "min_words = 5\n",
    "max_words = 15                        # threshold\n",
    "\n",
    "def count_words(sentence):\n",
    "    # Split on whitespace (handles multiple spaces)\n",
    "    return len(sentence.strip().split())\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile, \\\n",
    "     open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    \n",
    "    # Count total lines for tqdm progress bar\n",
    "    total_lines = sum(1 for _ in open(input_file, \"r\", encoding=\"utf-8\"))\n",
    "    infile.seek(0)\n",
    "    \n",
    "    for line in tqdm(infile, total=total_lines, desc=\"Filtering sentences\"):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # skip empty lines\n",
    "        \n",
    "        if min_words <= count_words(line) <= max_words :\n",
    "            outfile.write(line + \"\\n\")\n",
    "\n",
    "print(f\"\\n✅ Cleaned sentences saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b17f1d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 25852/25852 [00:00<00:00, 409382.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Sentences of exactly 10 words saved to 'marathi_10.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "input_file = \"marathi_clean.txt\"     # Source file\n",
    "output_file = \"marathi_10.txt\"       # Destination file\n",
    "target_length = 10                   # Minimum and trim length\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    words = sentence.strip().split()\n",
    "    if len(words) < target_length:\n",
    "        return None  # skip short sentences\n",
    "    return \" \".join(words[:target_length])  # trim to exactly 10 words\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile, \\\n",
    "     open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "\n",
    "    total_lines = sum(1 for _ in open(input_file, \"r\", encoding=\"utf-8\"))\n",
    "    infile.seek(0)\n",
    "\n",
    "    for line in tqdm(infile, total=total_lines, desc=\"Processing sentences\"):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # skip empty lines\n",
    "\n",
    "        processed = process_sentence(line)\n",
    "        if processed:\n",
    "            outfile.write(processed + \"\\n\")\n",
    "\n",
    "print(f\"\\n✅ Sentences of exactly 10 words saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da1367ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned file saved as temp.txt\n"
     ]
    }
   ],
   "source": [
    "input_file = \"marathi_short.txt\"\n",
    "output_file = \"temp.txt\"\n",
    "\n",
    "with open(input_file, \"rb\") as f:\n",
    "    raw_data = f.read()\n",
    "\n",
    "# Try decoding ignoring bad bytes\n",
    "clean_text = raw_data.decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(clean_text)\n",
    "\n",
    "print(\"✅ Cleaned file saved as\", output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
